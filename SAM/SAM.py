import asyncio
import os
import sys

from dotenv import load_dotenv
from ollama import Client, chat

from sam_config import SAM_personality, chat_history_system_prompt
from discord_functions.discord_message_helpers import session_chat_cache
from tools.vision.gemma_vision import vision_image_cleanup
from utility_scripts.system_logging import setup_logger
from utility_scripts.utility import split_response

from pathlib import Path

# configure logging
logger = setup_logger(__name__)

load_dotenv()
os.environ["OLLAMA_API_KEY"] = os.getenv("OLLAMA_API")

# model settings for easy swapping
sam_model_name = 'SAM'
sam_ollama_model = 'huihui_ai/deepseek-r1-abliterated'
sam_vision_model = 'gemma3'


def sam_create():
    try:
        client = Client()
        response = client.create(
            model=sam_model_name,
            from_=sam_ollama_model,
            system=SAM_personality,
            stream=False,
        )
        # print(f"# Client: {response.status}")
        logger.info(f"# Client: {response.status}")
        return

    except ConnectionError as e:
        logger.error('Ollama is not running!')
        sys.exit(1)  # Exit program with error code 1

    except Exception as e:
        # Catches any other unexpected errors
        logger.error("‚ùå An unexpected error occurred:", e)
        sys.exit(1)


# === Main Entry Point ===
async def sam_message(message_author_name, message_author_nickname, message_content, image_file=None, message_attachments=None):
    if image_file is None:
        llm_response = await sam_converse(message_author_name, message_author_nickname, message_content)
    else:
        llm_response = await sam_converse_image(message_author_name, message_author_nickname, message_content, image_file ,message_attachments)

    cleaned = llm_response.replace("'", "\\'")
    return split_response(cleaned)


async def sam_converse(user_name, user_nickname, user_input):
    current_session_chat_cache = session_chat_cache()

    chat_log = list(current_session_chat_cache)

    print("#########################################")
    for message in chat_log:
        print(message)
    print("#########################################")

    chat_history = "".join(chat_log)

    current_prompt = [{"role": "user", "content": chat_history}]

    full_prompt = (
        [
            {"role": "system", "content": SAM_personality},
            {"role": "system", "content": chat_history_system_prompt}
        ] +
        current_prompt
    )

    response = await asyncio.to_thread(
        chat,
        model=sam_model_name,
        messages=full_prompt,
        options={
            "num_ctx": 8192,
            'temperature': 0.5
        }
    )

    # return the message to main script
    return response.message.content


async def sam_converse_image(user_name, user_nickname, user_input, image_file, message_attachments):
    # Go one directory up
    parent_dir = Path(__file__).resolve().parent
    path = parent_dir / 'tools/vision/images_temp' / image_file

    attachments = message_attachments[0]["attachments"]

    # print(attachments)
    # print(f'Analyzing image ({image_file_name})...')
    logger.debug(f"Attachments: {attachments}")
    logger.info(f'Analyzing image ({image_file})...')

    response = await asyncio.to_thread(
        chat,
        model=sam_vision_model,
        messages=(
            [
                {"role": "system", "content": SAM_personality},
                {"role": "user", "content": user_input, 'images': [path]}
            ]
        ),
        options={
            "num_ctx": 8192
        }
    )

    # clean up image
    vision_image_cleanup(image_file)

    # return the message to main script
    return response.message.content

